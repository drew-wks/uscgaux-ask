{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following on the command line to pull the model using ollama:\n",
    "\n",
    "ollama pull deepseek-r1:32b\n",
    "ollama pull qwq\n",
    "\n",
    "ollama list\n",
    "\n",
    "ollama rm qwq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to run it at the command line\n",
    "\n",
    "ollama run qwq\n",
    "How many r's are in the word \"strawberry”?\n",
    "\n",
    "CTRL-C to stop\n",
    "then CTRL-D to exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/drew_wilkins/Drews_Files/Drew/Python/Repositories/ASK/.venv-main/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "\n",
    "# Confirm you're using the correct interpreter\n",
    "print(sys.executable)\n",
    "\n",
    "load_dotenv(\n",
    "    '/Users/drew_wilkins/Drews_Files/Drew/Python/Localcode/.env', override=True)\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from rag import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Would an LLM be able to help a firm identify companies that have innovative AI tech in its industry? If so, what info would the model need to conduct the scan, how would it conduct the scan and how might one evaluate its effectiveness in sreas such as recall and precision?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=\"milkey/QwQ-32B-0305:q4_K_M\",\n",
    "    # model=\"deepseek-r1:32b\",  # Specify the model you want to use\n",
    "    temperature=CONFIG[\"ASK_temperature\"],  # Set the temperature parameter\n",
    "    client_kwargs={\n",
    "        \"timeout\": 400,  # Set the timeout in seconds 40 for 8b\n",
    "        # Additional client configurations can be added here if needed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(user_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out whether an LLM can help a firm identify companies that have innovative AI tech in their industry. Let me break this down step by step.\n",
      "\n",
      "First, what exactly is an LLM? It's a Large Language Model, right? So it's trained on a lot of text data and can understand and generate human language. But how would it go about identifying companies with innovative AI tech?\n",
      "\n",
      "I guess the first thing I need to consider is what data the model would use. The user mentioned sources like news articles, patents, company filings, industry reports, academic papers, and social media. So, maybe the LLM could scan through these sources to find mentions of AI tech in different companies.\n",
      "\n",
      "Wait, but how does an LLM actually process this information? It can't interact with the internet in real-time, so it must have been trained on data up until a certain point. That might be a limitation because innovative tech is always evolving, and the model's knowledge would become outdated over time.\n",
      "\n",
      "Next, the model needs to scan through all these sources to identify companies. So perhaps it would look for keywords related to AI in each source. For example, if a company's annual report mentions investments in machine learning or natural language processing, that could be a signal. Or if there are patents filed by the company in AI-related areas, that might indicate innovation.\n",
      "\n",
      "But I'm not sure how the LLM would prioritize these sources. It might just go through them one by one without any real strategy. So maybe it's not as effective as a human analyst who can understand context and significance.\n",
      "\n",
      "Also, evaluating the effectiveness in terms of recall and precision is important. Recall refers to how many relevant companies the model finds, while precision is how accurate those findings are—few false positives or negatives.\n",
      "\n",
      "I think about how an LLM might miss some companies that have innovative AI tech because it doesn't understand the nuances or the deeper context. For example, a company's white paper might discuss a new AI application in detail, but if the model doesn't recognize certain technical terms or the specific context, it might overlook it.\n",
      "\n",
      "On the other hand, precision could be an issue if the model incorrectly categorizes companies without innovative tech as having it, leading to false positives. This could happen if it's matching on generic terms that don't necessarily indicate innovation.\n",
      "\n",
      "So, maybe combining the LLM's output with human analysis would be more effective. The LLM could provide a list of potential candidates based on keyword matches, and then humans can evaluate these companies more deeply, checking things like actual products, patents, and industry reputation.\n",
      "\n",
      "Another thought: how does the model handle ambiguous or context-dependent information? For instance, a company might use AI in customer service, but it's not necessarily innovative if it's just a basic chatbot. The model might not distinguish between levels of innovation unless it was specifically trained to do so.\n",
      "\n",
      "I also wonder about the scalability. As the number of companies grows, the LLM would need access to more data and better algorithms to handle larger datasets without losing effectiveness.\n",
      "\n",
      "In terms of evaluation metrics beyond recall and precision, maybe something like accuracy in identifying the level of innovation could be added. But that might complicate things further.\n",
      "\n",
      "Overall, while an LLM can assist by processing and extracting information from available sources, it's likely most effective when used as a tool alongside human analysts who can interpret the results accurately.\n",
      "</think>\n",
      "\n",
      "An LLM can assist a firm in identifying companies with innovative AI tech by leveraging its ability to process and analyze text data from various sources. However, its effectiveness is enhanced when combined with human analysis due to several considerations:\n",
      "\n",
      "1. **Data Sources**: The model would use sources like news articles, patents, company filings, industry reports, academic papers, and social media. It identifies keywords related to AI, such as investments in machine learning or natural language processing.\n",
      "\n",
      "2. **Limitations**: The LLM's knowledge is bounded by its training data, which may become outdated. It cannot interact in real-time with current information, potentially missing newer innovative developments.\n",
      "\n",
      "3. **Processing Strategy**: While it can scan through sources for keywords, its approach lacks strategic depth compared to human analysts who can contextualize findings.\n",
      "\n",
      "4. **Effectiveness Evaluation**:\n",
      "   - **Recall**: The model may miss companies due to limited context understanding.\n",
      "   - **Precision**: It may generate false positives by over-matching generic terms without indicating innovation.\n",
      "\n",
      "5. **Enhancement with Human Analysis**: Combining LLM output with human evaluation can improve accuracy, as humans can assess the significance and depth of AI applications beyond keyword matches.\n",
      "\n",
      "6. **Contextual Understanding**: The model might struggle to differentiate between basic AI use and innovative advancements unless specifically trained.\n",
      "\n",
      "7. **Scalability**: Handling larger datasets effectively requires advanced algorithms and continuous data updates.\n",
      "\n",
      "In conclusion, while an LLM is a valuable tool for initial screening, it's most effective when integrated with human expertise to ensure accurate and comprehensive identification of innovative AI tech companies.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(user_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response\n",
    "response = rag.rag(user_question)\n",
    "# response, enriched_question = rag.rag(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- \\ ASK \\ ------------------------------------- \n",
      "\n",
      "USER QUESTION: How Do I Dispose of My Old ODUs?\n",
      "\n",
      "ENRICHED QUESTION: How Do I Dispose of My Old ODUs?\n",
      "\n",
      "RETRIEVED DOCUMENTS:\n",
      "Auxiliary Manual, COMDTINST M16790.1G, page 501\n",
      "UNINSPECTED PASSENGER VESSEL EXAMINATION PROGRAM  LANT/PACAREAINST 16710.2  Enclosure (1) Requirements For Uninspected Passenger Vessels , page 29\n",
      "ALAUX 021/23 OPERATION DRY WATER, page 2\n",
      "ALAUX 12/15 TEMPORARY SUSPENSION OF UDC Istore   12JUN2015, page 0\n",
      "CH-2 TO MARINE SAFETY MANUAL VOLUME II, MATERIEL INSPECTION, COMDTINST M16000.7B, page 1214\n",
      "\n",
      "ANSWER: To dispose of your old Operational Dress Uniforms (ODUs), you should follow these guidelines:\n",
      "\n",
      "1. **Uniform Condition**: Ensure that the ODUs are in a condition that is appropriate for disposal. If they are still serviceable, consider donating them to a local veterans' organization or a charity that accepts military uniforms.\n",
      "\n",
      "2. **Disposal Process**: If the ODUs are no longer usable, they should be disposed of in a manner that respects the uniform's significance. This can include:\n",
      "   - Check with your local waste management for textile recycling options.\n",
      "   - If the uniform is heavily worn or damaged, it can be disposed of with regular trash, but it is recommended to cut the uniforms to prevent misuse.\n",
      "\n",
      "3. **Regulations**: Be aware that the Coast Guard does not specify a formal process for uniform disposal in the provided regulations, but maintaining respect for the uniform's symbolism is encouraged. \n",
      "\n",
      "4. **Contact Local Units**: If you're unsure about how to dispose of your ODUs or if you want more specific guidance, you may contact your local Coast Guard unit or Auxiliary representative for advice.\n",
      "\n",
      "Always ensure that any disposal method aligns with local regulations and environmental guidelines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--------- \\\\ ASK \\ ------------------------------------- \\n\")\n",
    "print(f\"USER QUESTION: {response['user_question']}\\n\")\n",
    "print(f\"ENRICHED QUESTION: {response['enriched_question']}\\n\")\n",
    "print(\"RETRIEVED DOCUMENTS:\")\n",
    "print(\"\\n\".join(\n",
    "    map(lambda doc: f\"{doc.metadata['title']}, page {doc.metadata['page']}\", response['context'])))\n",
    "print(f\"\\nANSWER: {response['answer']}\\n\")\n",
    "# print(f\"LLM BASED ANSWER ON:\")\n",
    "# print(\"\\n\".join(response['llm_sources'])) # this is for AnswersWithSources\n",
    "\n",
    "\n",
    "# short_source_list = rag.create_short_source_list(response)\n",
    "# long_source_list = rag.create_long_source_list(response)\n",
    "# print(f\"Short source list: {short_source_list}\")\n",
    "# print(f\"{long_source_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(response[\"sources\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text = response['answer']\n",
    "\n",
    "# Print the answer\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(\n",
    "    map(lambda doc: f\"{doc.metadata['title']}, page {doc.metadata['page']}\", response['context'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Response schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''{\n",
    "    'Question': 'string'\n",
    "    }\n",
    "\n",
    "response object:\n",
    "    {\n",
    "    'user_question': 'string'\n",
    "    'enriched_question': 'string'\n",
    "    'context': [list of Document objects]\n",
    "    'answer': {\n",
    "        'answer': 'string', \n",
    "        'llm_sources': ['string 1', 'string2']\n",
    "        }\n",
    "    }\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
