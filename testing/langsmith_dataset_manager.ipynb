{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from jsonschema import validate, ValidationError\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_create = \"omg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset schema\n",
    "inputs_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"question\"]\n",
    "}\n",
    "\n",
    "outputs_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"ground_truth_answer\": {\"type\": \"string\"},\n",
    "        \"ground_truth_sources\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"ground_truth_answer\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Check if dataset exists\n",
    "datasets = client.list_datasets()\n",
    "dataset = next((d for d in datasets if d.name == dataset_to_create), None)\n",
    "\n",
    "if dataset:\n",
    "    dataset_id = dataset.id\n",
    "    print(\n",
    "        f\"Dataset '{dataset_to_create}' already exists with ID: {dataset_id}\")\n",
    "else:\n",
    "    # If the dataset doesn't exist, create it\n",
    "    print(f\"Dataset '{dataset_to_create}' does not exist. Creating it now.\")\n",
    "    response = client.create_dataset(\n",
    "        dataset_name=dataset_to_create,\n",
    "        description=\"This is my imported dataset.\",\n",
    "        inputs_schema=inputs_schema,\n",
    "        outputs_schema=outputs_schema,\n",
    "    )\n",
    "    dataset_id = response.id\n",
    "    print(f\"Dataset '{dataset_to_create}' created with ID: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Evaluation Examples as JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/drew_wilkins/Downloads/dataset_gtv1_116f9507-0683-41a1-82aa-a905d6d8a225\"\n",
    "\n",
    "# Load your preprocessed CSV file\n",
    "csv_file = f'{filename}.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "# Ensure ground_truth_sources is formatted as an array\n",
    "def format_ground_truth_sources(value):\n",
    "    if pd.isna(value) or value == \"\" or value == \"NaN\":\n",
    "        return []  # Empty array for blank or NaN values\n",
    "    try:\n",
    "        # Parse JSON strings if already in JSON-like format\n",
    "        return json.loads(value)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Otherwise, wrap it in an array\n",
    "        return [value]\n",
    "\n",
    "\n",
    "# Ensure the ground_truth_sources column exists\n",
    "if 'output_ground_truth_sources' not in df.columns:\n",
    "    df['output_ground_truth_sources'] = \"\"\n",
    "\n",
    "# Apply formatting to the ground_truth_sources column\n",
    "if 'output_ground_truth_sources' in df.columns:\n",
    "    df['output_ground_truth_sources'] = df['output_ground_truth_sources'].apply(\n",
    "        lambda x: format_ground_truth_sources(x)\n",
    "    )\n",
    "\n",
    "# Save as JSON\n",
    "jsonl_file = f'{filename}.jsonl'  # Replace with your desired JSON file name\n",
    "with open(jsonl_file, 'w') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        jsonl_entry = {\n",
    "            \"input\": {\n",
    "                \"question\": row.get(\"input_question\", \"\")\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"ground_truth_answer\": row.get(\"output_ground_truth_answer\", \"\"),\n",
    "                \"ground_truth_sources\": row.get(\"output_ground_truth_sources\", [])\n",
    "            }\n",
    "        }\n",
    "        f.write(json.dumps(jsonl_entry) + '\\n')\n",
    "\n",
    "    print(f\"JSONL file saved to: {jsonl_file}\")\n",
    "jsonl_file\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add examples to Langsmith dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "# Use \"dataset_to_create\" if continuing from above\n",
    "dataset_name = \"ASK-groundtruth-v2\"\n",
    "\n",
    "# use \"jsonl_file\" if continuing from above\n",
    "jsonl_file_to_add = jsonl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'oyoy' retrieved successfully.\n",
      "\n",
      "Input Schema:\n",
      "{'properties': {'question': {'type': 'string'}},\n",
      " 'required': ['question'],\n",
      " 'type': 'object'}\n",
      "\n",
      "Output Schema:\n",
      "{'properties': {'ground_truth_answer': {'type': 'string'},\n",
      "                'ground_truth_sources': {'items': {'type': 'string'},\n",
      "                                         'type': 'array'}},\n",
      " 'required': ['ground_truth_answer'],\n",
      " 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# Fetch Dataset and Its Schema\n",
    "# List all datasets\n",
    "datasets = client.list_datasets()\n",
    "dataset = next((d for d in datasets if d.name == dataset_name), None)\n",
    "\n",
    "if dataset:\n",
    "    print(f\"Dataset '{dataset_name}' retrieved successfully.\\n\")\n",
    "\n",
    "    # Access the input and output schemas\n",
    "    input_schema = dataset.inputs_schema\n",
    "    output_schema = dataset.outputs_schema\n",
    "\n",
    "    print(\"Input Schema:\")\n",
    "    pprint(input_schema)\n",
    "    print(\"\\nOutput Schema:\")\n",
    "    pprint(output_schema)\n",
    "else:\n",
    "    print(f\"Dataset '{dataset_name}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 42 examples to dataset 'ASK-groundtruth-v2'.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Validate Examples Against the Schema\n",
    "def validate_against_schema(data, schema):\n",
    "    \"\"\"Validate a dictionary against a schema.\"\"\"\n",
    "    from jsonschema import validate, ValidationError\n",
    "    try:\n",
    "        validate(instance=data, schema=schema)\n",
    "        return True\n",
    "    except ValidationError as e:\n",
    "        print(f\"Validation error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "with open(jsonl_file_to_add, \"r\") as f:\n",
    "    rows = [json.loads(line) for line in f]\n",
    "\n",
    "valid_examples = []\n",
    "for row in rows:\n",
    "    if \"input\" in row and \"output\" in row:\n",
    "        if validate_against_schema(row[\"input\"], input_schema) and validate_against_schema(row[\"output\"], output_schema):\n",
    "            valid_examples.append(row)\n",
    "        else:\n",
    "            print(f\"Validation error for example: {row}\")\n",
    "    else:\n",
    "        print(f\"Row missing 'input' or 'output': {row}\")\n",
    "\n",
    "\n",
    "# Load Examples from JSONL Fil\n",
    "if valid_examples:\n",
    "    inputs = [example[\"input\"] for example in valid_examples]\n",
    "    outputs = [example[\"output\"] for example in valid_examples]\n",
    "\n",
    "    try:\n",
    "        response = client.create_examples(\n",
    "            dataset_id=None,\n",
    "            dataset_name=dataset_name,\n",
    "            inputs=inputs,\n",
    "            outputs=outputs,\n",
    "        )\n",
    "        print(\n",
    "            f\"Successfully added {len(inputs)} examples to dataset '{dataset_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error adding examples:\")\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No valid examples to add.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
