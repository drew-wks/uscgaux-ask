{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "examples_file_path = 'ASK-groundtruth-v3.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Eval set and parse it according to your ASK Eval set schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'examples_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read the JSONL file and convert it into a DataFrame\u001b[39;00m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mexamples_file_path\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'examples_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the JSONL file and convert it into a DataFrame\n",
    "data = []\n",
    "with open(examples_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"metadata\": [item[\"metadata\"][\"dataset_split\"] for item in data],\n",
    "        \"inputs_question\": [item[\"inputs\"][\"question\"] for item in data],\n",
    "        \"outputs_ground_truth_answer\": [\n",
    "            item[\"outputs\"][\"ground_truth_answer\"] for item in data\n",
    "        ],\n",
    "        \"outputs_ground_truth_sources\": [\n",
    "            item[\"outputs\"][\"ground_truth_sources\"] for item in data\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OPTIONAL**: Display the Eval Set in an Editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This process is not trusted! Input event monitoring will not be possible until it is added to accessibility clients.\n",
      "qt.qpa.fonts: Populating font family aliases took 139 ms. Replace uses of missing font family \"Consolas\" with one that exists to avoid this cost. \n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "os.environ[\"APPDATA\"] = \"/tmp\"\n",
    "\n",
    "from pandasgui import show\n",
    "\n",
    "# Suppress pandasgui logging\n",
    "logging.getLogger(\"pandasgui\").setLevel(logging.ERROR)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "gui = show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the LangSmith account to use based on API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/Localcode/.env')\n",
    "\n",
    "# choose the Langsmith account you want to use based on the API key\n",
    "client = Client(api_key=os.environ[\"LANGCHAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Eval Set to LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 17:00:23.260 Python[23191:249021] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-28 17:00:23.260 Python[23191:249021] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"ASK-groundtruth-v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'ASK-groundtruth-v3' has been successfully uploaded to LangSmith.\n"
     ]
    }
   ],
   "source": [
    "dataset = client.create_dataset(dataset_name)\n",
    "\n",
    "# Read and upload data from the JSONL file\n",
    "with open(examples_file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "\n",
    "        client.create_example(\n",
    "            dataset_name=dataset_name,\n",
    "            inputs={\"question\": entry[\"inputs\"][\"question\"]},  # Input question\n",
    "            outputs={\n",
    "                # Ground truth answer\n",
    "                \"ground_truth_answer\": entry[\"outputs\"][\"ground_truth_answer\"],\n",
    "                # Ground truth sources\n",
    "                \"ground_truth_sources\": entry[\"outputs\"][\"ground_truth_sources\"]\n",
    "            },\n",
    "            metadata={\n",
    "                # Dataset split information\n",
    "                \"dataset_split\": entry[\"metadata\"][\"dataset_split\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "print(f\"Dataset '{dataset_name}' has been successfully uploaded to LangSmith.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Datasets:\n",
      "- Name: one_example_easy, ID: 4a2a2222-1352-4b8c-8257-a8461e439097\n",
      "- Name: ASK-groundtruth-v3, ID: e44c3abc-7871-49a3-a388-6197d7c2dcf3\n"
     ]
    }
   ],
   "source": [
    "datasets = client.list_datasets()\n",
    "print(\"Available Datasets:\")\n",
    "for dataset in datasets:\n",
    "    print(f\"- Name: {dataset.name}, ID: {dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how many traces done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of traces in 'evaluators': 3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Set your LangSmith API key\n",
    "api_key = os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "\n",
    "# Define the API endpoints\n",
    "sessions_url = \"https://api.smith.langchain.com/sessions\"\n",
    "runs_url = \"https://api.smith.langchain.com/runs/query\"\n",
    "\n",
    "# Set the headers with your API key\n",
    "headers = {\n",
    "    \"x-api-key\": api_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Function to retrieve all sessions for a project\n",
    "\n",
    "\n",
    "def get_sessions(project_name):\n",
    "    params = {\"project_name\": project_name}\n",
    "    response = requests.get(sessions_url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\n",
    "            f\"Failed to retrieve sessions: {response.status_code} - {response.text}\")\n",
    "        return []\n",
    "\n",
    "# Function to retrieve root runs for a list of session IDs\n",
    "\n",
    "\n",
    "def get_root_runs(project_name, session_ids):\n",
    "    payload = {\n",
    "        \"project_name\": project_name,\n",
    "        \"is_root\": True,\n",
    "        \"session\": session_ids  # Pass the list of session IDs\n",
    "    }\n",
    "    response = requests.post(runs_url, json=payload, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\n",
    "            f\"Failed to retrieve runs: {response.status_code} - {response.text}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Main logic\n",
    "project_name = \"evaluators\"\n",
    "sessions = get_sessions(project_name)\n",
    "\n",
    "if not sessions:\n",
    "    print(\"No sessions found for the project.\")\n",
    "    exit()\n",
    "\n",
    "# Extract session IDs\n",
    "session_ids = [session[\"id\"] for session in sessions]\n",
    "\n",
    "# Retrieve root runs for all sessions\n",
    "root_runs = get_root_runs(project_name, session_ids)\n",
    "\n",
    "# Output the total number of root runs\n",
    "if root_runs:\n",
    "    print(f\"Total number of traces in '{project_name}': {len(root_runs)}\")\n",
    "else:\n",
    "    print(\"No root runs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%ollama` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
