{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "import streamlit as st\n",
    "\n",
    "load_dotenv('/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/.env')\n",
    "\n",
    "# Add the parent directory to sys.path so you can import your modules from a subdirectory\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from utils import rag\n",
    "from utils.rag import CONFIG\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LangSmith if you also want the traces\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = st.secrets[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_evaluator.ipynb on ASK main/local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "eval_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def validate_and_fix_json(raw_output: Any, required_fields: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates and fixes JSON output, ensuring required fields are present.\n",
    "\n",
    "    Args:\n",
    "        raw_output (Any): The raw JSON string or dictionary from the LLM.\n",
    "        required_fields (dict): A dictionary of required fields with their default values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Validated and fixed JSON output aligned with required fields.\n",
    "    \"\"\"\n",
    "    # If the input is already a dictionary, skip parsing\n",
    "    if isinstance(raw_output, dict):\n",
    "        parsed_response = raw_output\n",
    "    else:\n",
    "        try:\n",
    "            # Attempt to parse the JSON string\n",
    "            parsed_response = json.loads(raw_output)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError: {e}\")\n",
    "            print(f\"Raw output: {raw_output}\")\n",
    "\n",
    "            # Attempt common fixes\n",
    "            if isinstance(raw_output, str):\n",
    "                if raw_output.strip().endswith('\"'):\n",
    "                    raw_output = raw_output.rstrip('\"') + '\"}'\n",
    "                elif not raw_output.strip().endswith('}'):\n",
    "                    raw_output += '}'\n",
    "\n",
    "                # Retry parsing\n",
    "                try:\n",
    "                    parsed_response = json.loads(raw_output)\n",
    "                except json.JSONDecodeError as final_e:\n",
    "                    print(f\"Failed to fix JSON: {final_e}\")\n",
    "                    parsed_response = {}\n",
    "            else:\n",
    "                # If it's not a string and cannot be parsed, fallback to empty dict\n",
    "                parsed_response = {}\n",
    "\n",
    "    # Ensure required fields are present with default values\n",
    "    validated_response = {\n",
    "        key: parsed_response.get(key, default) for key, default in required_fields.items()\n",
    "    }\n",
    "\n",
    "    return validated_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader using dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grader Input: {'query': 'What is the capital of France?', 'ground_truth_answer': 'Paris', 'student_answer': 'Paris'}\n",
      "Grader Response: {'correctness': 1, 'explanation': \"The student's answer is 'Paris', which is the same as the context provided. Since the context states that the capital of France is Paris, the student's answer is factually accurate and correct.\"}\n",
      "Validated Response: {'correctness': 1, 'explanation': \"The student's answer is 'Paris', which is the same as the context provided. Since the context states that the capital of France is Paris, the student's answer is factually accurate and correct.\"}\n",
      "Evaluation Result: {'key': 'Accuracy', 'score': 1, 'value': 'Correct', 'comment': \"The student's answer is 'Paris', which is the same as the context provided. Since the context states that the capital of France is Paris, the student's answer is factually accurate and correct.\"}\n"
     ]
    }
   ],
   "source": [
    "grade_prompt_accuracy = hub.pull(\"drew-wks/cot_qa\")\n",
    "\n",
    "llm = ChatOpenAI(model=eval_model, temperature=0, tags=[\"accuracy_evaluator\"])\n",
    "\n",
    "answer_grader = grade_prompt_accuracy | llm\n",
    "\n",
    "# Example inputs for evaluation\n",
    "query = \"What is the capital of France?\"\n",
    "ground_truth_answer = \"Paris\"\n",
    "prediction = \"Paris\"\n",
    "\n",
    "# Prepare the input for the grader\n",
    "grader_input = {\n",
    "    \"query\": query,\n",
    "    \"ground_truth_answer\": ground_truth_answer,\n",
    "    \"student_answer\": prediction,\n",
    "}\n",
    "\n",
    "# Invoke the grader\n",
    "grader_response = answer_grader.invoke(grader_input)\n",
    "\n",
    "\n",
    "required_fields = {\n",
    "    \"correctness\": None,  # Default correctness value\n",
    "    \"explanation\": \"No explanation provided.\",  # Default explanation\n",
    "}\n",
    "\n",
    "\n",
    "# Validate and fix the grader response\n",
    "validated_response = validate_and_fix_json(grader_response, required_fields)\n",
    "\n",
    "# Extract evaluation results\n",
    "correctness = validated_response[\"correctness\"]\n",
    "explanation = validated_response[\"explanation\"]\n",
    "\n",
    "# Evaluation results object\n",
    "evaluation_result = {\n",
    "    \"key\": \"Accuracy\",\n",
    "    \"score\": correctness,\n",
    "    \"value\": \"Correct\" if correctness == 1 else \"Incorrect\",\n",
    "    \"comment\": explanation,\n",
    "}\n",
    "\n",
    "# Print objects for inspection\n",
    "print(\"Grader Input:\", grader_input)\n",
    "print(\"Grader Response:\", grader_response)\n",
    "print(\"Validated Response:\", validated_response)\n",
    "print(\"Evaluation Result:\", evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
