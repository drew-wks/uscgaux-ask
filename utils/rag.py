import os  # needed for local testing
import re
import logging
from typing import List, Tuple, Optional, Any
from typing_extensions import Annotated, TypedDict
import pandas as pd
from functools import lru_cache
import streamlit as st
from qdrant_client.http import models  # for running filters on the metadata
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama  # to test other LLMs
from langchain_core.prompts import ChatPromptTemplate
from langsmith import traceable  # RAG pipeline instrumentation platform
from .filter import build_retrieval_filter, catalog_filter
from uscgaux import stu
from .backends_bridge import (
    get_vectordb_connector,
    fetch_table_and_date_from_catalog,
)
from .chat_model_factory import create_chat_model


# st.secrets pulls from ~/.streamlit when run locally


# Config langchain_openai
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY_ASK"] # for langchain_openai.OpenAIEmbeddings

os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY_ASK"] # for openai client in cloud environment


logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO)

# retrieval filter function is defined in filter.py


def get_retriever(retrieval_filter: Optional[models.Filter]):
    """Create a document retriever from the active vector store with filters.

    Parameters
    ----------
    retrieval_filter : Optional[models.Filter]
        Optional Qdrant metadata filter generated by ``build_retrieval_filter``.

    Returns
    -------
    Any
        A retriever object from the configured vector store.
    """
    #  Config access (hard-fail on missing keys)
    config = stu.cached_load_config_by_context()
    
    search_type = config["RAG"]["RETRIEVAL"]["search_type"]
    k = config["RAG"]["RETRIEVAL"]["k"]  # e.g. 5
    fetch_k = config["RAG"]["RETRIEVAL"]["fetch_k"]
    lambda_mult = config["RAG"]["RETRIEVAL"]["lambda_mult"]

    vectordb = get_vectordb_connector()
    qdrant = vectordb.get_langchain_vectorstore()

    retriever = qdrant.as_retriever(
        search_type=search_type,
        search_kwargs={
            "k": k,
            "fetch_k": fetch_k,
            "lambda_mult": lambda_mult,
            "filter": retrieval_filter,
        },  # If None, no metadata filtering occurs
    )
    return retriever



BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
ACRONYMS_PATH = os.path.join(BASE_DIR, 'config', 'acronyms.csv')
TERMS_PATH = os.path.join(BASE_DIR, 'config', 'terms.csv')


@lru_cache(maxsize=64)
def get_retrieval_context_csv(file_path: str) -> dict:
    """
    Reads a CSV file into a dictionary.
    """
    df = pd.read_csv(file_path)

    if df.shape[1] < 2:
        raise ValueError("CSV must contain at least two columns")

    context_dict: dict = dict(zip(df.iloc[:, 0], df.iloc[:, 1]))
    return context_dict



# Define the enrichment function.
# traceable decorator is used to trace the function in Langsmith
# cache_data decorator is used to cache the function in Streamlit
@traceable(run_type="prompt")
#@st.cache_data
def enrich_question(user_question: str, acronyms_csv_path: str, terms_csv_path: str) -> str:
    """
    Enrich a user question by:
    - Expanding acronyms to their full forms
    - Adding explanations for defined terms

    Parameters
    ----------
    user_question : str
        The question text to enrich.
    acronyms_csv_path : str
        Path to CSV file containing acronym mappings (acronym â†’ full form).
    terms_csv_path : str
        Path to CSV file containing term mappings (term â†’ explanation).

    Returns
    -------
    str
        The enriched version of the user question.
    """
    acronyms_dict = get_retrieval_context_csv(acronyms_csv_path)
    terms_dict = get_retrieval_context_csv(terms_csv_path)

    enriched_question = user_question

    # Replace acronyms with full form
    for acronym, full_form in acronyms_dict.items():
        if pd.notna(acronym) and pd.notna(full_form):
            enriched_question = re.sub(
                r'\b' + re.escape(str(acronym)) + r'\b',
                str(full_form),
                enriched_question
            )

    # Add explanations for terms
    for term, explanation in terms_dict.items():
        if pd.notna(term) and pd.notna(explanation):
            if str(term) in enriched_question:
                enriched_question += f" ({str(explanation)})"

    return enriched_question


def create_prompt():
    system_prompt = (
        "The user is a {identity}."
        "Use the following pieces of context to answer the users question. "
        "INCLUDES ALL OF THE DETAILS IN YOUR RESPONSE, INDLUDING REQUIREMENTS AND REGULATIONS. "
        "National Workshops are required for boat crew, aviation, and telecommunications when they are offered. "
        "Include Auxiliary Core Training (AUXCT) for questions on certifications or officer positions. "
        "If you don't know the answer, just say I don't know. \n----------------\n{context}"
    )
    return ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{enriched_question}"),
    ])



# Function to format documents (doesn't require caching)
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def attach_catalog_metadata(docs: List, catalog_df: pd.DataFrame) -> List:
    """Merge catalog metadata into each document's metadata using ``pdf_id``.

    Parameters
    ----------
    docs : list
        Documents returned from Qdrant.
    catalog_df : pandas.DataFrame
        DataFrame containing catalog metadata with a ``pdf_id`` column.

    Returns
    -------
    list
        Documents with metadata updated from the catalog.
    """
    if "pdf_id" not in catalog_df.columns:
        return docs

    indexed = catalog_df.set_index("pdf_id")
    for doc in docs:
        pdf_id = doc.metadata.get("pdf_id")
        if pdf_id and pdf_id in indexed.index:
            row = indexed.loc[pdf_id]
            doc.metadata.update(row.to_dict())
    return docs



# Schema for llm responses
class AnswerWithSources(TypedDict):
    """An answer to the question, with sources."""
    answer: str
    sources: Annotated[
        List[str],
        ...,
        "List of sources and pages used to answer the question",
    ]



# --- Main RAG pipeline function ---
@traceable(run_type="chain")
def rag(
    user_question: str,
    timeout: int = 60,
    filter_conditions: Optional[dict[str, str | bool | None | list[str]]] = None,
    langsmith_extra: Optional[dict] = None,
) -> dict:
    """Run the RAG pipeline for a given question.

    Parameters
    ----------
    user_question : str
        The natural language question from the user.
    timeout : int, default=60
        Timeout for end-to-end processing.
    filter_conditions : Optional[dict[str, str | bool | None | list[str]]]
        Optional filter selections used to constrain retrieval.
    langsmith_extra : Optional[dict]
        Optional extra metadata for LangSmith tracing.

    Returns
    -------
    dict
        A response dictionary with keys: answer, sources, user_question,
        enriched_question, and context.
    """

    # Load generation settings from config (hard fail if missing)
    config = stu.cached_load_config_by_context()
    
    _model = config["RAG_ALL"]["generation_model"]  # e.g. "gpt-4o-mini"
    _temperature = config["RAG_ALL"]["temperature"]  # e.g. 0.7

    # new approach allows config to determin chat model
    llm = create_chat_model(config)

    llm_openai = ChatOpenAI(model=_model, max_retries=2, timeout=45, temperature=_temperature)

    # Optional local/test LLM (Ollama)
    llm_ollama = ChatOllama(
        model="deepseek-r1:8b",
        temperature=_temperature,
        client_kwargs={
            "timeout": 50,
        },
    )
    logger.info("ðŸ¤– Initiated RAG pipeline")
    # Enrich the question
    enriched_question = enrich_question(user_question, ACRONYMS_PATH, TERMS_PATH)
    logger.info("Question has been enriched")
    
    # Set the user identity
    identity = "Auxiliary member"

    # Initialize response dict
    response = {
        "answer": "âš ï¸ Something went wrong before answering.",
        "sources": [],
        "user_question": user_question,
        "enriched_question": enriched_question,
        "context": [],
    }
    
    # build filter (optional) and retriever
    logger.info("Received filter conditions from user: %s", filter_conditions)
    
    catalog_df, _ = fetch_table_and_date_from_catalog()
    allowed_ids = catalog_filter(catalog_df, filter_conditions)
    retrieval_filter = build_retrieval_filter(
        filter_conditions,
        allowed_pdf_ids=allowed_ids,
    )
    logger.info("Created retrieval filter: %s", retrieval_filter)

    # Prepare tracing metadata from config
    _rag_all = config["RAG_ALL"]  # attach full RAG_ALL as retriever metadata
    retriever = get_retriever(retrieval_filter=retrieval_filter).with_config(metadata=_rag_all)
    
    
    # Retrieve relevant documents using the enriched question
    context: list = []
    try:
        context = retriever.invoke(enriched_question)
        logger.info("ðŸ“„ Retrieved context: %d documents", len(context))
        if not context:
            response["answer"] = (
                "â—ï¸I couldn't find any documents that match your filters. Please try relaxing your filters."
            )
            return response

        # Attach catalog metadata based on pdf_id
        context = attach_catalog_metadata(context, catalog_df)
        response["context"] = context

    except Exception as e:
        logger.exception("Retriever Error: %s", e)
    
    # Prepare the prompt input
    try:
        prompt = create_prompt()
        prompt_input = {
            "identity": identity,
            "enriched_question": enriched_question,
            "context": format_docs(context),  # list of documents from vectorstore
        }
        llm_response = llm.invoke(prompt.format(**prompt_input))
        response["answer"] = llm_response.content
        response["sources"] = [doc.metadata.get("title", "") for doc in context]
        logger.info("ðŸ§  Received LLM response")
    except Exception as e:
        logger.exception("LLM Error: %s", e)
        response["answer"] = f"âš ï¸ There was a problem generating a response: {e}"
    return response


# Adapter for running evals to LangSmith. No longer used
def rag_for_eval(input: dict) -> dict:
    # Accepts a input dict from langsmith.evaluation.LangChainStringEvaluator
    # Outputs a dictionary with one key which is the answer
    user_question = input["Question"]
    response = rag(user_question)
    return {"answer": response["answer"]}


def create_source_lists(response: dict, catalog_df: pd.DataFrame) -> Tuple:
    """
    Creates and returns both the short and long source lists as strings.

    Args:
        response (dict): Contains a 'context' key with a list of document objects.

    Returns:
        tuple: (short_source_list, long_source_list) where:
            - short_source_list is a string with a markdown reference for each document.
            - long_source_list is a string with a detailed markdown reference for each document.
    """
    short_source_markdown_list = []
    long_source_markdown_list = []


    indexed = catalog_df.set_index("pdf_id") if "pdf_id" in catalog_df.columns else None

    for i, doc in enumerate(response['context'], start=1):
        pdf_id = doc.metadata.get('pdf_id')
        row = indexed.loc[pdf_id] if indexed is not None and pdf_id in indexed.index else {}

        title = row.get('title', '')
        date = row.get('issue_date', '')[:4]
        link = row.get('link', '')
        page = str(int(doc.metadata.get('page', 0)) + 1)
        publication_number = (lambda x: (s := str(x).strip()) and s or " ")(row.get('publication_number'))
        scope = (lambda x: " " if not x or str(x).strip().lower() == "national" else str(x).strip())(row.get('scope'))
        unit = (lambda x: (s := str(x).strip()) and s or " ")(row.get('unit'))
        organization = (lambda x: f"Issuer: {x.strip()}" if x and x.strip() else None)(row.get('organization'))


        short_source_markdown_list.append(f"  *{scope} {unit} {title} [{date}], page {page}\n  ")
        
        page_content = doc.page_content  
        long_source_markdown_list.append(
            f"**Reference {i}:**  \n    {scope} {unit} {title} [{date}], page {page}  \n  {link}    \n  {publication_number}  \n  {organization}\n   {scope} {unit}\n\n  "
            f"{page_content}\n\n  "
            f"***  "
        )
    
    short_source_list = '\n'.join(short_source_markdown_list)
    long_source_list = '  \n'.join(long_source_markdown_list)
    
    return short_source_list, long_source_list
